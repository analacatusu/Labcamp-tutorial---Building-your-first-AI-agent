{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Jupyter Notebook with LLM in LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary packages if you haven't\n",
    "# !pip install langgraph openai\n",
    "\n",
    "import openai\n",
    "from langgraph import LangGraph, Node, Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set up OpenAI API key\n",
    "openai.api_key = 'your-openai-api-key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a LangGraph instance\n",
    "graph = LangGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define the Context (stores user preferences and generated responses)\n",
    "context = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Define the LLM Node (GPT-3)\n",
    "def query_with_llm(context):\n",
    "    # Retrieve user input from the context\n",
    "    user_input = context.get(\"user_input\")\n",
    "    \n",
    "    # Use GPT-3 to generate a response based on the user input\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"User asked: {user_input}\\nProvide a helpful and informative response.\",\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Get the generated text from the response\n",
    "    generated_text = response.choices[0].text.strip()\n",
    "    \n",
    "    # Save the generated response in context\n",
    "    context.set(\"llm_response\", generated_text)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "llm_node = Node(\n",
    "    name=\"Query with LLM\",\n",
    "    action=query_with_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Define a Node to Display the LLM Response\n",
    "display_llm_response_node = Node(\n",
    "    name=\"Display LLM Response\",\n",
    "    action=lambda context: print(\"LLM Response: \", context.get(\"llm_response\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Define a Node for Asking User Input (Preference or Query)\n",
    "ask_user_input_node = Node(\n",
    "    name=\"Ask User for Input\",\n",
    "    action=lambda context: context.set(\"user_input\", input(\"Ask me something: \"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Set the flow of the graph (edges)\n",
    "ask_user_input_node.add_edge(llm_node)\n",
    "llm_node.add_edge(display_llm_response_node)\n",
    "\n",
    "# Add the nodes to the graph\n",
    "graph.add_node(ask_user_input_node)\n",
    "graph.add_node(llm_node)\n",
    "graph.add_node(display_llm_response_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Run the graph\n",
    "def run_graph():\n",
    "    ask_user_input_node.execute(context)\n",
    "    llm_node.execute(context)\n",
    "    display_llm_response_node.execute(context)\n",
    "\n",
    "# Run the graph to interact with the LLM\n",
    "run_graph()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
